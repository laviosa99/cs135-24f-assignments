{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 for CS 135 in Fall 2023\n",
    "\n",
    "[Main assignment instructions here](https://www.cs.tufts.edu/cs/135/2024f/Assignments/hw2.html)\n",
    "\n",
    "This notebook is specifically for the report related to Problem 2\n",
    "\n",
    "# Problem 2: Binary Classifier for Cancer-Risk Screening "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "* You will need to finish **Code Task** for **Problem 1** before working on this notebook. \n",
    "* Your outputs, including tables and figures, don't need to be exactly the same as our sample outputs, but we do expect something as clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('notebook', style='whitegrid', font_scale=1.25)\n",
    "\n",
    "# autoload changes in other files, so you don't have to restart the Jupyter kernel each time you make a change to the imported code.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import student-edited code \n",
    "\n",
    "Remember, you should have *completed* the Code Tasks for Problem 1 first. Note that this file needs to be in the same directory/folder as `binary_metrics.py` in order to function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('binary_metrics.py'):\n",
    "    raise ImportError(\"CANNOT FIND binary_metrics.py. Make sure you run this notebook in same directory as your .py files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binary_metrics import (\n",
    "    calc_ACC, calc_TPR, calc_PPV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import helper code (will work as provided, no edits needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threshold_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confusion_matrix import calc_confusion_matrix_for_probas_and_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provided function for computing mean binary cross entropy\n",
    "\n",
    "Here, we provide a *completed* function you can use as-is for Problem 1.\n",
    "\n",
    "Remember, we want the *base-2* cross entropy:\n",
    "\n",
    "$$\n",
    "BCE(y, p) = - y \\log_2 p - (1-y) \\log_2(1-p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_binary_cross_entropy_from_probas(ytrue_N, yproba1_N):\n",
    "    ''' Compute mean binary cross entropy\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    ytrue_N : 1D array, size (n_examples,) = (N,)\n",
    "    yproba1_N : 1D array, size (n_examples,) = (N,)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean_bce : float\n",
    "        mean binary cross entropy across all N examples\n",
    "    '''\n",
    "    return sklearn.metrics.log_loss(ytrue_N, yproba1_N, labels=[0,1]) / np.log(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that BCE loss is high if true class is 1 but probability is low\n",
    "calc_mean_binary_cross_entropy_from_probas([1.], [0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that BCE loss is exactly 1 if true class is 1 but probability is 0.5\n",
    "calc_mean_binary_cross_entropy_from_probas([1.], [0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that BCE loss is close to zero if true class is 1 but probability is 0.99\n",
    "calc_mean_binary_cross_entropy_from_probas([1.], [0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have downloaded data and your directory is correct\n",
    "DATA_DIR = os.path.join('../data_cancer') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3 feature version of x arrays\n",
    "x_tr_M3 = np.loadtxt(os.path.join(DATA_DIR, 'x_train.csv'), delimiter=',', skiprows=1)\n",
    "x_va_N3 = np.loadtxt(os.path.join(DATA_DIR, 'x_valid.csv'), delimiter=',', skiprows=1)\n",
    "x_te_N3 = np.loadtxt(os.path.join(DATA_DIR, 'x_test.csv'), delimiter=',', skiprows=1)\n",
    "\n",
    "for label, x in [('train', x_tr_M3), ('valid', x_va_N3), ('test', x_te_N3)]:\n",
    "    print(\"Loaded %6s : shape %s\" % (label, x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_shape = x_va_N3.shape\n",
    "M_shape = x_tr_M3.shape\n",
    "\n",
    "N = N_shape[0]\n",
    "M = M_shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Step 1A: Data Exploration\n",
    "\n",
    "**TODO** : Load outcomes **y** arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load outcomes y arrays\n",
    "# Hint: Follow the way we import the x arrays above.\n",
    "y_tr_M = np.hstack([np.zeros(M//2), np.ones(M//2)]) # TODO fixme\n",
    "y_va_N = np.hstack([np.zeros(N//2), np.ones(N//2)]) # TODO fixme\n",
    "y_te_N = np.hstack([np.zeros(N//2), np.ones(N//2)]) # TODO fixme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2 feature version of x arrays\n",
    "x_tr_M2 = x_tr_M3[:, :2].copy()\n",
    "x_va_N2 = x_va_N3[:, :2].copy()\n",
    "x_te_N2 = x_te_N3[:, :2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1 \n",
    "\n",
    "Create and print a pandas table summarizing some basic properties of the provided training set, validation set, and test set:\n",
    "\n",
    "* Row 1 'total count': how many total examples are in each set?\n",
    "* Row 2 'positive label count': how many examples have a positive label (means cancer)?\n",
    "* Row 3 'fraction positive' : what fraction (between 0 and 1) of the examples have cancer?\n",
    "\n",
    "Your result should be 3 by 3 containing 'total count', 'positive label count' and 'fraction positive' of training, valid and test sets. An example of the output looks like this (note that the numbers in this table are examples, not the numbers that should be in your table):\n",
    "\n",
    "|                               | train  |valid    |test|\n",
    "|:-|-:|-:|-:|\n",
    "|num. total examples            |567.000  |123.000  |123.000\n",
    "|num. positive examples         |56.000   |23.000   |23.000\n",
    "|fraction of positive examples  |0.123    |0.123    |0.123\n",
    "\n",
    "All results should **keep 3 digits**. We set a pandas display option to ensure that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: make a pandas dataframe with the correct data properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_df = pd.DataFrame() # TODO fixme\n",
    "table1_df.index = ['num. total examples', 'num. positive examples', 'fraction of positive examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 3)\n",
    "print(table1_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implementation Step 1B: The predict-0-always baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: predict zero for all test data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_yhat_te_N = np.random.RandomState(0).randint(low=0, high=2, size=N) # TODO fixme\n",
    "print(baseline_yhat_te_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_te_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_confusion_matrix_for_probas_and_threshold(y_te_N, baseline_yhat_te_N, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Use the printed information from the previous code cell to calculate the accuracy of baseline. \n",
    "\n",
    "Keep 3 digits in your PDF report for short Answer 1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_acc = -1.0 # TODO: calculate the accuracy of baseline.\n",
    "print(\"Baseline has accuracy: \", baseline_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Answer 1a\n",
    "\n",
    "What *accuracy* does the \"predict-0-always\" classifier get on the *test* set (report to 3 decimal places)?\n",
    "(You should see a pretty high number). Why isn't this classifier \"good enough\" to use in our screening task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Step 1C : Logistic Regression with F=2 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete each line marked TODO fixme in the codeblock below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_grid = np.logspace(-9, 6, 31)\n",
    "\n",
    "# We will fit a separate logistic regression for each C value in the C_grid\n",
    "# And store that classifier's performance metrics (lower is better)\n",
    "# So we can compare and select the best C in the future steps.\n",
    "\n",
    "model_F2_list = list()\n",
    "\n",
    "# Allocate lists for storing BCE metrics\n",
    "tr_bce_list = list()\n",
    "va_bce_list = list()\n",
    "\n",
    "# Allocate lists for storing ERROR RATE metrics\n",
    "tr_err_list = list()\n",
    "va_err_list = list()\n",
    "\n",
    "# Remember, we justified BCE for training our classifier by saying\n",
    "# it provides an *upper bound* on the error rate.\n",
    "\n",
    "# Loop over C values, fit models, record metrics\n",
    "for C in C_grid:\n",
    "    # TODO: Follow the instruction in HW2 and train the model lr_F2\n",
    "    # Part a: Initialize a LogisticRegression classifier with desired C value\n",
    "    # Part b: train the model with the 2-feature dataset\n",
    "    lr_F2 = sklearn.linear_model.LogisticRegression() # TODO fixme\n",
    "    lr_F2.fit(x_tr_M2[::5], y_tr_M[::5]) # TODO fixme\n",
    "    \n",
    "    model_F2_list.append(lr_F2)\n",
    "    \n",
    "    yproba1_tr_M = lr_F2.predict_proba(x_tr_M2)[:,1] # The probability of predicting class 1 on the training set\n",
    "    yproba1_va_N = lr_F2.predict_proba(x_va_N2)[:,1] # The probability of predicting class 1 on the validation set\n",
    "    \n",
    "    # Compute error rate aka zero-one loss\n",
    "    my_tr_err = sklearn.metrics.zero_one_loss(y_tr_M, yproba1_tr_M >= 0.5)\n",
    "    my_va_err = sklearn.metrics.zero_one_loss(y_va_N, yproba1_va_N >= 0.5)\n",
    "    tr_err_list.append(my_tr_err)\n",
    "    va_err_list.append(my_va_err)\n",
    "    \n",
    "    # TODO: using the calc_mean_binary_cross_entropy_from_probas() function from above:\n",
    "    # Part c: calculate the binary cross entropy (bce) on the training set\n",
    "    # Part d: calculate the binary cross entropy (bce) on the validation set\n",
    "    my_tr_bce = my_tr_err + 0.1 # TODO fixme\n",
    "    my_va_bce = my_va_err + 0.15 # TODO fixme\n",
    "    # Save bce for future selection on Models.\n",
    "    tr_bce_list.append(my_tr_bce) \n",
    "    va_bce_list.append(my_va_bce) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log10(C_grid), tr_bce_list, 'bs-', label='train BCE')\n",
    "plt.plot(np.log10(C_grid), va_bce_list, 'rs-', label='valid BCE')\n",
    "\n",
    "plt.plot(np.log10(C_grid), tr_err_list, 'b:', label='train err')\n",
    "plt.plot(np.log10(C_grid), va_err_list, 'r:', label='valid err')\n",
    "\n",
    "plt.ylabel('error')\n",
    "plt.xlabel(\"$\\log_{10} C$\");\n",
    "plt.legend(bbox_to_anchor=(1.5, 0.5)) # make legend outside plot\n",
    "plt.ylim([0, 0.7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Point:\n",
    "\n",
    "If your code is correct, you should produce exactly the same figure when you run the cell above as:\n",
    "\n",
    "**\"Checkpoint1C.jpg\"** in the same folder as your starter code.\n",
    "![image.png](Checkpoint1C.JPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Searched over these C values:\")\n",
    "print(C_grid)\n",
    "\n",
    "print(\"Recorded these BCE loss values on val set\")\n",
    "print(va_bce_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Find the best C with the samllest cross entropy loss on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best C with the smallest cross entropy loss on the validation set\n",
    "best_C__lrF2 = C_grid[-1] # TODO fixme\n",
    "\n",
    "print(\"Best C value for F2 model:\")\n",
    "print(best_C__lrF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Load the model that was rated 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lrF2 = model_F2_list[-1] # TODO fixme\n",
    "\n",
    "print(\"Best model has coefficient values:\")\n",
    "print(best_lrF2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the prediction of your best model for 2-feature data on the training, valid, and test set. \n",
    "# Return the posibility of predicting true\n",
    "# We'll use them for the ROC curve\n",
    "bestlrF2_yproba1_tr_M = best_lrF2.predict_proba(x_tr_M2)[:,1]\n",
    "bestlrF2_yproba1_va_N = best_lrF2.predict_proba(x_va_N2)[:,1]\n",
    "bestlrF2_yproba1_te_N = best_lrF2.predict_proba(x_te_N2)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Step 1D : Logistic Regression with F=3 dataset\n",
    "**TODO**: \n",
    "* Repeat Step 1C for 3-feature Dataset to find the best C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "* Select the best C and retrieve the best model. You should name the model as \"**best_lrF3**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "* Return the posibility of predicting true on training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Return the posibility of predicting true on training, validation and test set.\n",
    "# Hint: follow what's done in the last code block of Step 1C.\n",
    "\n",
    "prng = np.random.RandomState(101)\n",
    "bestlrF3_yproba1_tr_M = prng.rand(M) # TODO fixme\n",
    "bestlrF3_yproba1_va_N = prng.rand(N) # TODO fixme\n",
    "bestlrF3_yproba1_te_N = prng.rand(N) # TODO fixme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Step 1E : Comparing Models using ROC Analysis\n",
    "**TODO**:\n",
    "Follow the instruction to produce the figure for your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the instruction of HW2 and produce Figure 1 for your report:\n",
    "plt.subplots(nrows=1, ncols=1, figsize=(5,5));\n",
    "\n",
    "# TODO Use provided data and predictions on the Validation set\n",
    "# Produce the ROC Curve utilizing `sklearn.metrics.roc_curve' within plt defined above.\n",
    "# To start, read the official Document and examples of 'sklearn.metrics.roc_curve'. \n",
    "\n",
    "# Read HW2 instructions carefully for plot style (line type, line color, etc.)\n",
    "\n",
    "plt.title(\"ROC on Validation Set\");\n",
    "plt.xlabel('false positive rate');\n",
    "plt.ylabel('true positive rate');\n",
    "plt.legend(loc='lower right');\n",
    "B = 0.01\n",
    "plt.xlim([0 - B, 1 + B]);\n",
    "plt.ylim([0 - B, 1 + B]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1 for the report: Comparing models using ROC analysis\n",
    "**TODO**:\n",
    "Show the figure above in your report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Answer 1b\n",
    "\n",
    "Compare the two models in terms of their ROC curves from Figure 1. Does one dominate the other in terms of overall performance across all thresholds, or are there some threshold regimes where the 2-feature model is preferred and other regimes where the 3-feature model is preferred? Which model do you recommend for the task at hand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a decision threshold\n",
    "\n",
    "Now that we've compared models, we need to decide on a classification *threshold* to obtain a binary decision from probabilities. \n",
    "\n",
    "To get candidate threshold values, use the helper function `compute_perf_metrics_across_thresholds` in the starter code file [threshold_selection.py](https://github.com/tufts-ml-courses/cs135-23f-assignments/blob/main/hw2/threshold_selection.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Step 1F\n",
    "\n",
    "For the classifier from 1D above (LR for 3-features), calculate performance metrics using the default threshold of `y_proba < 0.5`. Produce the confusion matrix and calculate the TPR and PPV. \n",
    "*Tip: Remember that we have implemented helper functions for you in `confusion_matrix.py`.*\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Use F=3 LR model, Use default 0.5 threshold\n",
    "* Produce the confusion matrix\n",
    "* Return TPR and PPV on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default threshold (0.5): chosen thr = 0.5000, tpr = 0.0000, ppv = 0.0000,\n"
     ]
    }
   ],
   "source": [
    "best_thr_default = 0.5\n",
    "tpr = 0.0 # TODO fixme\n",
    "ppv = 0.0 # TODO fixme\n",
    "\n",
    "print(f\"Default threshold (0.5): chosen thr = {best_thr_default :.4f}, tpr = {tpr :.4f}, ppv = {ppv :.4f},\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Step 1G\n",
    "\n",
    "For the classifier from 1D above (LR for 3-features), compute performance metrics across all candidate thresholds on the validation set (use `compute_perf_metrics_across_thresholds`). Then, pick the threshold that *maximizes TPR while satisfying PPV >= 0.98* on the validation set.\n",
    "If there's a tie for the maximum TPR, chose the threshold corresponding to a higher PPV. \n",
    "\n",
    "Remember, you pick this threshold based on the *validation* set, then later you'll evaluate it on the *test* set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Finish the code block to get the best threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the performance metrics across many thresholds\n",
    "thresh_grid, va_perf_grid = threshold_selection.compute_perf_metrics_across_thresholds(\n",
    "    y_va_N, bestlrF3_yproba1_va_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold that makes TPR as large as possible, while satisfying PPV >= 0.98\n",
    "# TODO Find the the largest TPR while PPV >= 0.98\n",
    "\n",
    "best_thr_tpr = 0.5 # TODO fixme\n",
    "tpr1 = 0.0 # TODO fixme\n",
    "ppv1 = 0.0 # TODO fixme\n",
    "\n",
    "print(f\"Figure2 column 2: chosen thr = {best_thr_tpr :.4f}, tpr = {tpr1 :.4f}, ppv = {ppv1 :.4f},\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Using the chosen **best_thr** above\n",
    "* Produce the confusion matrix on the test set\n",
    "* Return TPR and PPV on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Step 1H\n",
    "\n",
    "Now using the same F=3 LR model, pick a threshold to maximize PPV s.t. TPR >= 0.98\n",
    "\n",
    "TODO: \n",
    "Using a similar logistic from **Step 1F** above\n",
    "* Choose threshold that makes PPV as large as possible, while satisfying TPR >= 0.98\n",
    "* Produce the confusion matrix using the chosen threshold\n",
    "* Return TPR and PPV on the test set using the chosen threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Find the largest PPV while TPR >= 0.98\n",
    "best_thr_ppv = 0.5 # TODO fixme\n",
    "tpr2 = 0.0 # TODO fixme\n",
    "ppv2 = 0.0 # TODO fixme\n",
    "\n",
    "print(\"Figure2 column 3: chosen thr = %.4f, tpr = %.4f, ppv = %.4f\" % (best_thr_ppv, tpr2, ppv2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Answer 1c\n",
    "\n",
    "By carefully reading the confusion matrices above, for *each thresholding strategy* how many subjects in the test set are saved from unnecessary biopsies that would be done in current practice?\n",
    "\n",
    "*Hint*: You can assume that currently, the hospital would have done a biopsy on every patient in the test set. Your goal is to build a classifier that improves on this practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Answer 1d\n",
    "\n",
    "Among the 3 possible thresholding strategies, which strategy best meets the stated goals of stakeholders in this screening task: avoid life-threatening mistakes whenever possible, while also eliminating unnecessary biopsies?\n",
    "What fraction of current biopsies might be avoided if this strategy was adopted by the hospital? \n",
    "\n",
    "*Hint*: You can also assume the test set is a reasonable representation of the true population of patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "1. How many hours, approximatley, did this homework take you?\n",
    "2. What did you feel was most confusing about this homework?\n",
    "3. Did you recieve help on this homework from any classmates, stack overflow threads, AI tools, Youtube videos, etc. for this homework? If so, please list them here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
